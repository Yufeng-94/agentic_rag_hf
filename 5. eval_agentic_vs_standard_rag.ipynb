{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fdf8615-de78-4ff3-bc85-4636be7f4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from smolagents import models, LiteLLMModel, ToolCallingAgent, LogLevel\n",
    "\n",
    "from src.utils import get_openai_api_key\n",
    "from src.vector_db_cfg import (\n",
    "    sports_cfg,\n",
    "    finance_cfg,\n",
    "    movie_cfg,\n",
    "    unified_cfg,\n",
    ")\n",
    "from src.retrieval_tool import RetrieverTool\n",
    "from src.evaluation import evaluate_rag_response\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eca6c1e-ac7a-4f7d-9704-26df05ec905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c41b1-c54d-4d0a-8783-91e3123132a5",
   "metadata": {},
   "source": [
    "# 1. Connect to LLM\n",
    "\n",
    "Connect to LLM using OpenAI api.\n",
    "\n",
    "Create a `llm.env` in root folder:\n",
    "```\n",
    "OPENAI_API_KEY='<your-openai-api-key>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecbc501-c76d-4f36-bc4d-3be6bdceed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load openai api key into environment\n",
    "get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caccd48e-b4d3-4e0b-8b3c-10babb6969eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_ID = \"o3-mini\"\n",
    "\n",
    "llm = LiteLLMModel(model_id=LLM_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c720ce6-5f5d-4a90-971a-bbdfdddc3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n",
      "model_id: o3-mini-2025-01-31\n"
     ]
    }
   ],
   "source": [
    "# test llm\n",
    "test_msg = models.ChatMessage(role=\"user\", content=\"hello!\")\n",
    "\n",
    "response = llm.generate([test_msg])\n",
    "\n",
    "print(response.content)\n",
    "print(\"model_id:\", response.raw.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdf4cd-456e-4ac5-adca-ae5e41bb672b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Create Retrieval Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9f0fd1-8a88-4988-a9ab-b26a50066a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedding  model\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b7a8a6-8f66-4235-87fa-964212d332e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f5e245-2c98-4fb2-a382-d9e3dc1c31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vector database\n",
    "db_dir = os.path.join(data_dir, \"faiss\")\n",
    "\n",
    "sports_vector_db = FAISS.load_local(\n",
    "    os.path.join(db_dir, \"sports\"),\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "finance_vector_db = FAISS.load_local(\n",
    "    os.path.join(db_dir, \"finance\"),\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "movie_vector_db = FAISS.load_local(\n",
    "    os.path.join(db_dir, \"movie\"),\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e8cd27-4655-4acf-8964-65ff867d4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_tool = RetrieverTool(vectordb=sports_vector_db, cfg=sports_cfg)\n",
    "finance_tool = RetrieverTool(vectordb=finance_vector_db, cfg=finance_cfg)\n",
    "movie_tool = RetrieverTool(vectordb=movie_vector_db, cfg=movie_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da82968-316a-4a08-a386-94a527ea38e1",
   "metadata": {},
   "source": [
    "# 3. Create RAG AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f79ae27e-bcd8-435f-938d-92e59be1ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using smolagents ToolCallingAgent\n",
    "agent = ToolCallingAgent(\n",
    "    model=llm,\n",
    "    tools=[sports_tool, finance_tool, movie_tool],\n",
    "    max_steps=10, \n",
    "    verbosity_level=LogLevel.ERROR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a170c2-c7c7-4199-9e3d-101561018fc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Run Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d302379d-41be-45b5-9106-2b38901c41ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>question_type</th>\n",
       "      <th>static_or_dynamic</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47859020-9974-4c81-a897-96594beca8fb</td>\n",
       "      <td>movie</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>static</td>\n",
       "      <td>how many family movies were there that came ou...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80365e4f-795e-4039-8afb-b7a8e8d54285</td>\n",
       "      <td>movie</td>\n",
       "      <td>post-processing</td>\n",
       "      <td>static</td>\n",
       "      <td>what was the average budget for all movies in ...</td>\n",
       "      <td>$147,375,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id domain    question_type  \\\n",
       "0  47859020-9974-4c81-a897-96594beca8fb  movie      aggregation   \n",
       "1  80365e4f-795e-4039-8afb-b7a8e8d54285  movie  post-processing   \n",
       "\n",
       "  static_or_dynamic                                              query  \\\n",
       "0            static  how many family movies were there that came ou...   \n",
       "1            static  what was the average budget for all movies in ...   \n",
       "\n",
       "         answer  \n",
       "0           109  \n",
       "1  $147,375,000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_metadata = pd.read_csv(os.path.join(data_dir, \"question_metadata.csv\"))\n",
    "question_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b43ed8-5ac4-4524-aba6-58525b075978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query prompt template\n",
    "enhanced_query = \"\"\"You are a helpful AI assistant.\n",
    "You solve tasks step-by-step, using tools when needed.\n",
    "\n",
    "Available tools are three retriever tools:\n",
    "1. sports_retriever: contains knowledge about sports, such as basketball, football, etc.\n",
    "2. finance_retriever: contains knowledge about earnings reports, market trends, stock performance, and economic indicators\n",
    "3. movie_retriever: contains knowledge about film synopses, box office data, cast information, and critical reception.\n",
    "\n",
    "Follow this loop:\n",
    "1. Think about the problem.\n",
    "2. If a tool is needed, call it with the right input.\n",
    "3. Observe the result.\n",
    "4. Repeat until you can give a final answer.\n",
    "\n",
    "You can only directly answer daily conversation questions, such as \"hello\", \"could you help me find questions?\"; when answering factual question, you have to use retriever tools to get answers.\n",
    "If you can not find relevant information from given retriever tools, just response that you did not find relevant information.\n",
    "\n",
    "\n",
    "User question: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829be797-3dfc-4260-a2c1-e9032aa55c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_rag_output = []\n",
    "\n",
    "for _, row in question_metadata.iterrows():\n",
    "    int_id = row['interaction_id']\n",
    "    question = row['query']\n",
    "    query = enhanced_query.format(question)\n",
    "\n",
    "    response = agent.run(query)\n",
    "    response_dict = {\n",
    "        'interaction_id': int_id,\n",
    "        'agentic_response': response,\n",
    "    }\n",
    "    \n",
    "    agentic_rag_output.append(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6a9ef7-4195-46fd-bc18-abfdb4d23aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>agentic_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47859020-9974-4c81-a897-96594beca8fb</td>\n",
       "      <td>I did not find relevant information about the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80365e4f-795e-4039-8afb-b7a8e8d54285</td>\n",
       "      <td>After reviewing the details from the retrieved...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id  \\\n",
       "0  47859020-9974-4c81-a897-96594beca8fb   \n",
       "1  80365e4f-795e-4039-8afb-b7a8e8d54285   \n",
       "\n",
       "                                    agentic_response  \n",
       "0  I did not find relevant information about the ...  \n",
       "1  After reviewing the details from the retrieved...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_output_df = pd.DataFrame(agentic_rag_output)\n",
    "agentic_rag_output_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c786f5a-2188-4520-a5e2-09110d377d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_rag_output_df.to_csv(\n",
    "    os.path.join(data_dir, 'agentic_rag_output.csv'), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884836d-9b5a-46eb-8624-c0210e87b05f",
   "metadata": {},
   "source": [
    "# 5. Run Standard RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5809d77f-300c-43ce-999a-0b8efc216d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unified retriever tool\n",
    "unified_vector_db = FAISS.load_local(\n",
    "    os.path.join(db_dir, \"unified\"),\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "unified_retriever = RetrieverTool(vectordb=unified_vector_db, cfg=unified_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fad739ad-d85d-44ad-b1bc-aba92b239750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template\n",
    "prompt_template = \"\"\"You are a helpful AI assistant.\n",
    "You answer question based on supporting documents below, give a comprehensive answer to the question.\n",
    "\n",
    "If you can not find relevant information from given documents, just response that you did not find relevant information.\n",
    "Do not give answers based on you own knowledge.\n",
    "\n",
    "\n",
    "User question: {}\n",
    "\n",
    "Documents:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16926ed6-72b3-4a41-a2a8-d5f8d6fcb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect response from standard RAG\n",
    "standard_rag_output = []\n",
    "for _, row in question_metadata.iterrows():\n",
    "    # get question\n",
    "    question = row['query']\n",
    "    int_id = row['interaction_id']\n",
    "    \n",
    "    # retrieve context\n",
    "    context = unified_retriever(question)\n",
    "    \n",
    "    # fit context into prompt\n",
    "    prompt = prompt_template.format(question, context)\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # generate response based on prompt\n",
    "    response = llm.generate(message)\n",
    "    response_dict = {\n",
    "        'interaction_id': int_id,\n",
    "        'standard_response': response.content,\n",
    "    }\n",
    "\n",
    "    standard_rag_output.append(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6508699-3a5c-4c5a-977e-18a9bc3cfe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>standard_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47859020-9974-4c81-a897-96594beca8fb</td>\n",
       "      <td>I looked through the provided documents, but n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80365e4f-795e-4039-8afb-b7a8e8d54285</td>\n",
       "      <td>Based on the documents, especially Document 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id  \\\n",
       "0  47859020-9974-4c81-a897-96594beca8fb   \n",
       "1  80365e4f-795e-4039-8afb-b7a8e8d54285   \n",
       "\n",
       "                                   standard_response  \n",
       "0  I looked through the provided documents, but n...  \n",
       "1  Based on the documents, especially Document 1,...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_rag_output_df = pd.DataFrame(standard_rag_output)\n",
    "standard_rag_output_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "127d5855-2949-4e1a-8f92-95ba826449a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_rag_output_df.to_csv(\n",
    "    os.path.join(data_dir, 'standard_rag_output.csv'), \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ee3c3-0e1d-4503-b309-c88615997d3c",
   "metadata": {},
   "source": [
    "# 6. Setup a Judge Model to Evaluate Answers from Both RAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f16cdec2-5b07-491e-ae22-cb4f40235c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm response: Hello! How can I assist you today?\n",
      "model_id: gpt-4o-2024-08-06\n"
     ]
    }
   ],
   "source": [
    "JUDGE_LLM_MODEL_ID = \"gpt-4o\" # select gpt-4o as judging model\n",
    "judge_llm = LiteLLMModel(model_id=JUDGE_LLM_MODEL_ID)\n",
    "\n",
    "test_msg = models.ChatMessage(role=\"user\", content=\"hello!\")\n",
    "response = judge_llm.generate([test_msg])\n",
    "print(\"llm response:\", response.content)\n",
    "print(\"model_id:\", response.raw.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "212196a4-9b84-4b0c-8e43-b469cf8f0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt and eval criteria for judge model\n",
    "base_prompt = \"\"\"You are an impartial evaluator tasked with scoring a model's answer.\n",
    "\n",
    "You will receive:\n",
    "1. **Question** – the original user query.\n",
    "2. **Reference Answer** – the correct, authoritative answer for comparison.\n",
    "3. **Model Answer** – the answer generated by the system under evaluation.\n",
    "\n",
    "Your task:\n",
    "Compare the Model Answer against the Reference Answer ONLY. Do not use external knowledge.\n",
    "\n",
    "### Scoring Criteria (1–5)\n",
    "\n",
    "**1. Faithfulness (Factual Accuracy)**\n",
    "- 5 = Fully matches the reference answer; no incorrect or contradictory information.\n",
    "- 4 = Mostly matches; one minor deviation that does not change the meaning.\n",
    "- 3 = Partially correct; includes both correct and incorrect elements.\n",
    "- 2 = Mostly incorrect; some overlap with the reference but major errors present.\n",
    "- 1 = Completely incorrect or contradictory.\n",
    "\n",
    "**2. Relevance**\n",
    "- 5 = Entire answer addresses the question directly.\n",
    "- 4 = Mostly relevant; minor irrelevant detail.\n",
    "- 3 = Partially relevant; significant portion unrelated to the question.\n",
    "- 2 = Mostly off-topic with only slight relevance.\n",
    "- 1 = Entirely irrelevant.\n",
    "\n",
    "**3. Completeness**\n",
    "- 5 = Fully answers the question; no significant omissions.\n",
    "- 4 = Minor omission but core answer is complete.\n",
    "- 3 = Several relevant points missing.\n",
    "- 2 = Very incomplete; most key points missing.\n",
    "- 1 = Does not address the core aspects of the question.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format\n",
    "Return only a valid JSON object:\n",
    "{\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"relevance\": <1-5>,\n",
    "  \"completeness\": <1-5>,\n",
    "  \"justification\": \"Brief explanation comparing the model answer to the reference answer.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Refernce Answer:\n",
    "{reference_answer}\n",
    "\n",
    "### Model Answer:\n",
    "{model_answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd4f1892-1e07-445a-a7bd-59eb37632151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>question_type</th>\n",
       "      <th>static_or_dynamic</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>agentic_response</th>\n",
       "      <th>standard_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47859020-9974-4c81-a897-96594beca8fb</td>\n",
       "      <td>movie</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>static</td>\n",
       "      <td>how many family movies were there that came ou...</td>\n",
       "      <td>109</td>\n",
       "      <td>I did not find relevant information about the ...</td>\n",
       "      <td>I looked through the provided documents, but n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80365e4f-795e-4039-8afb-b7a8e8d54285</td>\n",
       "      <td>movie</td>\n",
       "      <td>post-processing</td>\n",
       "      <td>static</td>\n",
       "      <td>what was the average budget for all movies in ...</td>\n",
       "      <td>$147,375,000</td>\n",
       "      <td>After reviewing the details from the retrieved...</td>\n",
       "      <td>Based on the documents, especially Document 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id domain    question_type  \\\n",
       "0  47859020-9974-4c81-a897-96594beca8fb  movie      aggregation   \n",
       "1  80365e4f-795e-4039-8afb-b7a8e8d54285  movie  post-processing   \n",
       "\n",
       "  static_or_dynamic                                              query  \\\n",
       "0            static  how many family movies were there that came ou...   \n",
       "1            static  what was the average budget for all movies in ...   \n",
       "\n",
       "         answer                                   agentic_response  \\\n",
       "0           109  I did not find relevant information about the ...   \n",
       "1  $147,375,000  After reviewing the details from the retrieved...   \n",
       "\n",
       "                                   standard_response  \n",
       "0  I looked through the provided documents, but n...  \n",
       "1  Based on the documents, especially Document 1,...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge collected answers from both agentic and standard RAGs\n",
    "merged_question_metadata = question_metadata.merge(\n",
    "    agentic_rag_output_df, \n",
    "    on=['interaction_id']\n",
    ").merge(\n",
    "    standard_rag_output_df,\n",
    "    on=['interaction_id']\n",
    ")\n",
    "\n",
    "merged_question_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "562bbd95-d61c-4154-83d9-fba35d65643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect eval scores\n",
    "judge_model_output = []\n",
    "\n",
    "for _, row in merged_question_metadata.iterrows():\n",
    "    record = {}\n",
    "    int_id = row['interaction_id']\n",
    "    question = row['query']\n",
    "    true_answer = row['answer']\n",
    "    agentic_answer = row['agentic_response']\n",
    "    standard_answer = row['standard_response']\n",
    "    \n",
    "    record.update({'interaction_id': int_id})\n",
    "\n",
    "    # eval agentic RAG\n",
    "    agentic_prompt = base_prompt + prompt_template.format(\n",
    "        question=question,\n",
    "        reference_answer=true_answer,\n",
    "        model_answer=agentic_answer,\n",
    "    )\n",
    "    \n",
    "    agentic_eval_dict = evaluate_rag_response(\n",
    "        prompt=agentic_prompt,\n",
    "        judge_llm=judge_llm,\n",
    "        eval_name=\"agentic\",\n",
    "    )\n",
    "    record.update(agentic_eval_dict)\n",
    "    \n",
    "    standard_prompt = base_prompt + prompt_template.format(\n",
    "        question=question,\n",
    "        reference_answer=true_answer,\n",
    "        model_answer=standard_answer,\n",
    "    )\n",
    "    \n",
    "    standard_eval_dict = evaluate_rag_response(\n",
    "        prompt=standard_prompt,\n",
    "        judge_llm=judge_llm,\n",
    "        eval_name=\"standard\"\n",
    "    )\n",
    "    record.update(standard_eval_dict)\n",
    "\n",
    "    judge_model_output.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61c11741-6c67-4d3d-9c27-1c6cb3b5ed7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>agentic_faithfulness</th>\n",
       "      <th>agentic_relevance</th>\n",
       "      <th>agentic_completeness</th>\n",
       "      <th>agentic_justification</th>\n",
       "      <th>standard_faithfulness</th>\n",
       "      <th>standard_relevance</th>\n",
       "      <th>standard_completeness</th>\n",
       "      <th>standard_justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47859020-9974-4c81-a897-96594beca8fb</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The model answer does not provide the number o...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The model answer does not provide the exact nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80365e4f-795e-4039-8afb-b7a8e8d54285</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>The model answer is mostly faithful, providing...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>The model answer provides a detailed breakdown...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id  agentic_faithfulness  \\\n",
       "0  47859020-9974-4c81-a897-96594beca8fb                     1   \n",
       "1  80365e4f-795e-4039-8afb-b7a8e8d54285                     3   \n",
       "\n",
       "   agentic_relevance  agentic_completeness  \\\n",
       "0                  2                     1   \n",
       "1                  5                     5   \n",
       "\n",
       "                               agentic_justification  standard_faithfulness  \\\n",
       "0  The model answer does not provide the number o...                      2   \n",
       "1  The model answer is mostly faithful, providing...                      4   \n",
       "\n",
       "   standard_relevance  standard_completeness  \\\n",
       "0                   1                      1   \n",
       "1                   5                      5   \n",
       "\n",
       "                              standard_justification  \n",
       "0  The model answer does not provide the exact nu...  \n",
       "1  The model answer provides a detailed breakdown...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_model_output_df = pd.DataFrame(judge_model_output)\n",
    "judge_model_output_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b64dec4-a360-4c82-8ec7-ed449e9eb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model_output_df.to_csv(\n",
    "    os.path.join(data_dir, 'judge_model_output.csv'), \n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd5a74b5-f80c-477e-bc80-bf58c5ed9839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agentic_faithfulness     3.02\n",
       "agentic_relevance        4.04\n",
       "agentic_completeness     3.46\n",
       "standard_faithfulness    2.68\n",
       "standard_relevance       3.70\n",
       "standard_completeness    2.92\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_metrics = judge_model_output_df[[\n",
    "    'agentic_faithfulness', \n",
    "    'agentic_relevance',\n",
    "    'agentic_completeness', \n",
    "    'standard_faithfulness', \n",
    "    'standard_relevance', \n",
    "    'standard_completeness',\n",
    "]].mean()\n",
    "\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e558cebf-2c60-4115-ac7c-84a809579f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic RAG improvments:\n",
      "- faithfulness: 12.69%\n",
      "- relevance: 9.19%\n",
      "- completeness: 18.49%\n"
     ]
    }
   ],
   "source": [
    "# metric improvements\n",
    "print(\"Agentic RAG improvments:\")\n",
    "for m in ['faithfulness', 'relevance', 'completeness']:\n",
    "    improved = (avg_metrics[f\"agentic_{m}\"] - avg_metrics[f\"standard_{m}\"]) / avg_metrics[f\"standard_{m}\"]\n",
    "    improved_pct = round(improved*100, 2)\n",
    "    print(\n",
    "        f\"- {m}:\", \n",
    "        f\"{improved_pct}%\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d6851-f1bb-4e99-90bc-f7ed6e0070e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
